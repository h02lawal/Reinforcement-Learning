{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe5c4e0-8466-431b-9ae6-6b6cc737afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "Learned Q-Table:\n",
      "[[ 3.12193797  4.58      ]\n",
      " [ 3.12196903  6.2       ]\n",
      " [ 4.57999575  8.        ]\n",
      " [ 6.19996566 10.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Test Run:\n",
      "\n",
      "Path to treasure: \n",
      "left 0, right 1, right 2, right 3, right 4 ðŸŽ‰"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ðŸ›  Step 1: Setup the Environment\n",
    "# Here, we define the \"world\" where the agent moves.\n",
    "states = [0, 1, 2, 3, 4]  # The environment has 5 different states (positions).\n",
    "actions = [0, 1]  # Two possible actions: 0 = move left, 1 = move right.\n",
    "treasure_state = 4  # The goal is to reach state 4, where the treasure is located!\n",
    "\n",
    "# ðŸ“„ Step 2: Initialize the Q-Table\n",
    "# The Q-table is a lookup table where the agent \"remembers\" how good an action is at a given state.\n",
    "q_table = np.zeros((len(states), len(actions)))  # Initially, all Q-values are 0 (no knowledge yet).\n",
    "print(q_table)  # Let's print the empty table to see what it looks like.\n",
    "\n",
    "# ðŸ”§ Define the \"rules of learning\" (hyperparameters)\n",
    "learning_rate = 0.1        # How fast the agent updates its knowledge (0 = slow, 1 = fast).\n",
    "discount_factor = 0.9      # How much future rewards matter compared to immediate rewards.\n",
    "epsilon = 0.2              # Probability of exploring random actions (instead of picking the best known one).\n",
    "episodes = 1000            # How many full games (episodes) the agent will play to learn.\n",
    "\n",
    "# ðŸŽ¯ Step 3: Define How Rewards Are Given\n",
    "# Rewards motivate the agent! Good actions should give high rewards, bad actions low or negative rewards.\n",
    "def get_reward(state):\n",
    "    if state == treasure_state:\n",
    "        return 10  # Big reward for reaching the treasure (goal achieved! ðŸŽ‰)\n",
    "    else:\n",
    "        return -1  # Small penalty for every move to encourage reaching the goal faster.\n",
    "\n",
    "# ðŸ§  Step 4: The Main Q-Learning Algorithm (Learning happens here)\n",
    "for episode in range(episodes):  # Repeat the learning process many times to get better.\n",
    "    state = 0  # Always start at state 0 at the beginning of each episode.\n",
    "\n",
    "    while state != treasure_state:  # Keep moving until the agent reaches the goal.\n",
    "        # ðŸŽ² Decide whether to Explore (try new random actions) or Exploit (choose best-known action)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # Explore: try something random to learn more.\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit: choose the best action known so far.\n",
    "\n",
    "        # ðŸš¶â€â™‚ï¸ Take the chosen action\n",
    "        if action == 0:  # If action is 0, move left (can't move beyond 0).\n",
    "            next_state = max(0, state - 1)\n",
    "        else:  # If action is 1, move right (can't move beyond the treasure).\n",
    "            next_state = min(treasure_state, state + 1)\n",
    "\n",
    "        # ðŸŽ Receive a reward based on the next state\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # ðŸ›  Update the Q-table to remember what happened\n",
    "        old_value = q_table[state, action]  # What we thought before about this action.\n",
    "        next_max = np.max(q_table[next_state])  # The best value for the next state.\n",
    "\n",
    "        # ðŸ§ª New Q-value: a mix of what we knew + what we just learned\n",
    "        new_value = (1 - learning_rate) * old_value + learning_rate * (reward + discount_factor * next_max)\n",
    "        q_table[state, action] = new_value  # Update the table with the new value.\n",
    "\n",
    "        # ðŸ”„ Move the agent to the next state\n",
    "        state = next_state\n",
    "\n",
    "# ðŸ“Š Step 5: After all episodes, print the Learned Q-Table\n",
    "print(\"Learned Q-Table:\")\n",
    "print(q_table)  # Now the Q-table should have smart values showing the best actions!\n",
    "\n",
    "# ðŸ§ª Step 6: Test Run: Let's See How the Agent Moves Now\n",
    "print(\"\\nTest Run:\")\n",
    "state = 0  # Start from the beginning\n",
    "path = [state]  # We'll record the path taken.\n",
    "#ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
    "indexPath = [state]\n",
    "\n",
    "while state != treasure_state:  # Until the treasure is reached\n",
    "    action = np.argmax(q_table[state])  # Choose the best action (no randomness now).\n",
    "    ind = np.unravel_index(np.argmax(q_table[state], axis=None), q_table[state].shape)\n",
    "    if action == 0:\n",
    "        state = max(0, state - 1)\n",
    "    else:\n",
    "        state = min(treasure_state, state + 1)\n",
    "    path.append(state)  # Add the new state to the path.\n",
    "    indexPath.append(ind)\n",
    "\n",
    "# ðŸŽ‰ Show the path the agent learned to follow to reach the goal!\n",
    "print(\"\\nPath to treasure: \")\n",
    "count = 0\n",
    "for num in range(len(path)):\n",
    "    if indexPath[num] == 0:\n",
    "        print(f\"left {path[num]}, \", end=\"\")\n",
    "    else:\n",
    "        if count == len(path)-1:\n",
    "            print(f\"right {path[num]} ðŸŽ‰\", end=\"\")\n",
    "        else:\n",
    "            print(f\"right {path[num]}, \", end=\"\")\n",
    "    count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b2ee6-e1fc-45f3-948d-a9df4678fd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
