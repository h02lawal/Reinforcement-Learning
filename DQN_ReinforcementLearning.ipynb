{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087b0c4d-3ac4-4a45-88ce-2a427b866b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:23:33.123114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Done!\n",
      "Path to treasure: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Step 1: Define the simple environment\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = [0, 1]\n",
    "treasure_state = 4\n",
    "\n",
    "# Step 2: Build the Neural Network\n",
    "def build_q_network():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(1,)))         # Input layer: one value (the state)\n",
    "    model.add(layers.Dense(16, activation='relu'))  # Hidden layer\n",
    "    model.add(layers.Dense(2))                   # Output layer: two actions (left, right)\n",
    "    return model\n",
    "\n",
    "q_network = build_q_network()\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.2\n",
    "episodes = 1000\n",
    "\n",
    "# Helper: get reward\n",
    "def get_reward(state):\n",
    "    if state == treasure_state:\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Step 3: DQN Training Loop\n",
    "for episode in range(episodes):\n",
    "    state = 0  # Start at state 0\n",
    "\n",
    "    while state != treasure_state:\n",
    "        state_tensor = np.array([[state]], dtype=np.float32)\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            q_values = q_network.predict(state_tensor, verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        # Take action\n",
    "        if action == 0:\n",
    "            next_state = max(0, state - 1)\n",
    "        else:\n",
    "            next_state = min(treasure_state, state + 1)\n",
    "\n",
    "        reward = get_reward(next_state)\n",
    "\n",
    "        # Prepare tensors\n",
    "        next_state_tensor = np.array([[next_state]], dtype=np.float32)\n",
    "\n",
    "        # Predict Q-values\n",
    "        q_values = q_network(state_tensor)\n",
    "        next_q_values = q_network(next_state_tensor)\n",
    "\n",
    "        # Target for current action\n",
    "        target_q = q_values.numpy()\n",
    "        target_q[0, action] = reward + gamma * np.max(next_q_values)\n",
    "\n",
    "        # Train step\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_pred = q_network(state_tensor, training=True)\n",
    "            loss = loss_fn(target_q, q_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, q_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "print(\"\\nTraining Done!\")\n",
    "\n",
    "# Step 4: Test the agent\n",
    "state = 0\n",
    "path = [state]\n",
    "while state != treasure_state:\n",
    "    state_tensor = np.array([[state]], dtype=np.float32)\n",
    "    q_values = q_network.predict(state_tensor, verbose=0)\n",
    "    action = np.argmax(q_values[0])\n",
    "    if action == 0:\n",
    "        state = max(0, state - 1)\n",
    "    else:\n",
    "        state = min(treasure_state, state + 1)\n",
    "    path.append(state)\n",
    "\n",
    "print(f\"Path to treasure: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fde369-0c87-4849-a349-bdd6d0feec1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
